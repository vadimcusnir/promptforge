




‚ö†Ô∏è **SECURITY WARNING**: This file contains EXAMPLE data only!
- All sensitive data has been anonymized
- DO NOT use in production without proper sanitization
- This is documentation/example code only

INFRASTRUCTURƒÇ CORE ‚Äì PRIMUL VECTOR
√éncepi cu backend-ul. Nu cu designul. Nu cu interfa»õa.
ORDIN EXECUTABIL:

Supabase Schema ‚Äì baza ontologicƒÉ

Tabele: users, resources, plans, entitlements, prompt_runs
RLS policies pentru gating Pilot/Pro/Enterprise
Storage pentru .pdf/.json exports


Next.js 14 App Router ‚Äì arhitectura nucleu
/app/layout.tsx    ‚Üí meta, fonts, providers
/app/page.tsx      ‚Üí landing orchestral 
/app/generator/    ‚Üí 7-D configurator + module grid
/app/api/          ‚Üí server routes (export, gpt-test, entitlements)

Module System ‚Äì 50 unitƒÉ»õi industriale M01-M50

lib/modules.ts ‚Üí catalog static cu metadata
Vectori 7-D: domain, scale, urgency, timeline, output, tone, constraints
Fiecare modul = input spec + output template + KPI target


Stripe Integration ‚Äì monetizarea imediatƒÉ

3 planuri: Pilot (free), Pro (29‚Ç¨), Enterprise (custom)
Webhooks pentru subscription updates
Gating la export .pdf/.json



DE CE ACEASTƒÇ SECVEN»öƒÇ:
Schema ‚Üí structurƒÉ ‚Üí logicƒÉ ‚Üí interfa»õƒÉ ‚Üí styling.
Fiecare layer valideazƒÉ urmƒÉtorul. Nu construie»ôti front-end fƒÉrƒÉ backend functional. Nu faci design fƒÉrƒÉ logica modulelor definitƒÉ.
URMƒÇTORUL PAS CONCRET:
Creezi db/schema.sql cu tabelele core »ôi RLS policies. Apoi lib/modules.ts cu primele 10 module validate.



-- ============================================================================
-- SUPABASE SCHEMA - PromptForge Infrastructure
-- ============================================================================

-- Enable necessary extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgcrypto";

-- ============================================================================
-- CORE TYPES & ENUMS
-- ============================================================================

CREATE TYPE plan_type AS ENUM ('pilot', 'pro', 'enterprise');
CREATE TYPE export_format AS ENUM ('txt', 'md', 'json', 'pdf', 'zip');
CREATE TYPE run_status AS ENUM ('pending', 'running', 'completed', 'failed', 'cancelled');
CREATE TYPE module_vector AS ENUM ('strategic', 'content', 'technical', 'sales', 'operational', 'creative', 'analytical');

-- ============================================================================
-- USERS & AUTHENTICATION
-- ============================================================================

-- Users table (extends auth.users)
CREATE TABLE public.users (
    id UUID REFERENCES auth.users(id) PRIMARY KEY,
    email TEXT NOT NULL UNIQUE,
    full_name TEXT,
    avatar_url TEXT,
    plan plan_type DEFAULT 'pilot' NOT NULL,
    credits_remaining INTEGER DEFAULT 10,
    stripe_customer_id TEXT UNIQUE,
    stripe_subscription_id TEXT UNIQUE,
    subscription_status TEXT,
    trial_ends_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- User preferences and settings
CREATE TABLE public.user_preferences (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    user_id UUID REFERENCES public.users(id) ON DELETE CASCADE,
    default_vectors JSONB DEFAULT '{}',
    export_preferences JSONB DEFAULT '{}',
    ui_preferences JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(user_id)
);

-- ============================================================================
-- MODULES SYSTEM (M01-M50)
-- ============================================================================

-- Modules catalog
CREATE TABLE public.modules (
    id TEXT PRIMARY KEY, -- M01, M02, etc.
    title TEXT NOT NULL,
    description TEXT NOT NULL,
    vector module_vector NOT NULL,
    difficulty INTEGER CHECK (difficulty BETWEEN 1 AND 5),
    estimated_tokens INTEGER,
    input_schema JSONB NOT NULL,
    output_template TEXT NOT NULL,
    guardrails TEXT[],
    kpi_target TEXT,
    sample_output TEXT,
    is_active BOOLEAN DEFAULT true,
    requires_plan plan_type DEFAULT 'pilot',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Module usage analytics
CREATE TABLE public.module_usage (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    module_id TEXT REFERENCES public.modules(id),
    usage_count INTEGER DEFAULT 0,
    avg_score DECIMAL(3,1),
    success_rate DECIMAL(3,2),
    last_used TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(module_id)
);

-- ============================================================================
-- PROMPT RUNS & HISTORY
-- ============================================================================

-- Prompt generation runs
CREATE TABLE public.prompt_runs (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    user_id UUID REFERENCES public.users(id) ON DELETE CASCADE,
    module_id TEXT REFERENCES public.modules(id),
    session_config JSONB NOT NULL, -- 7-D configuration
    generated_prompt TEXT,
    optimized_prompt TEXT,
    ai_score INTEGER CHECK (ai_score BETWEEN 0 AND 100),
    test_result JSONB,
    status run_status DEFAULT 'pending',
    execution_time_ms INTEGER,
    token_usage INTEGER,
    cost_usd DECIMAL(8,4),
    error_message TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    completed_at TIMESTAMPTZ
);

-- User session configurations (7-D specs)
CREATE TABLE public.session_configs (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    user_id UUID REFERENCES public.users(id) ON DELETE CASCADE,
    name TEXT,
    config JSONB NOT NULL, -- domain, scale, urgency, timeline, output, tone, constraints
    is_default BOOLEAN DEFAULT false,
    used_count INTEGER DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ============================================================================
-- EXPORTS & STORAGE
-- ============================================================================

-- Export requests and tracking
CREATE TABLE public.exports (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    user_id UUID REFERENCES public.users(id) ON DELETE CASCADE,
    prompt_run_id UUID REFERENCES public.prompt_runs(id) ON DELETE CASCADE,
    format export_format NOT NULL,
    file_path TEXT, -- Supabase Storage path
    file_size INTEGER,
    download_count INTEGER DEFAULT 0,
    expires_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Bundle exports (zip containing multiple formats)
CREATE TABLE public.export_bundles (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    user_id UUID REFERENCES public.users(id) ON DELETE CASCADE,
    prompt_run_ids UUID[],
    bundle_name TEXT NOT NULL,
    file_path TEXT,
    file_size INTEGER,
    formats export_format[],
    download_count INTEGER DEFAULT 0,
    expires_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ============================================================================
-- ENTITLEMENTS & GATING
-- ============================================================================

-- Feature entitlements per plan
CREATE TABLE public.entitlements (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    plan plan_type NOT NULL,
    feature_key TEXT NOT NULL,
    is_enabled BOOLEAN DEFAULT false,
    limit_value INTEGER, -- NULL = unlimited
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(plan, feature_key)
);

-- User-specific feature overrides
CREATE TABLE public.user_entitlements (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    user_id UUID REFERENCES public.users(id) ON DELETE CASCADE,
    feature_key TEXT NOT NULL,
    is_enabled BOOLEAN DEFAULT false,
    limit_value INTEGER,
    expires_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(user_id, feature_key)
);

-- Usage tracking for gated features
CREATE TABLE public.feature_usage (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    user_id UUID REFERENCES public.users(id) ON DELETE CASCADE,
    feature_key TEXT NOT NULL,
    usage_count INTEGER DEFAULT 1,
    usage_date DATE DEFAULT CURRENT_DATE,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(user_id, feature_key, usage_date)
);

-- ============================================================================
-- ANALYTICS & METRICS
-- ============================================================================

-- Daily usage aggregates
CREATE TABLE public.daily_usage (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    date DATE NOT NULL,
    total_runs INTEGER DEFAULT 0,
    total_users INTEGER DEFAULT 0,
    total_exports INTEGER DEFAULT 0,
    avg_score DECIMAL(3,1),
    revenue_usd DECIMAL(10,2) DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE(date)
);

-- User activity tracking
CREATE TABLE public.user_activity (
    id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
    user_id UUID REFERENCES public.users(id) ON DELETE CASCADE,
    action TEXT NOT NULL,
    resource_type TEXT,
    resource_id TEXT,
    metadata JSONB DEFAULT '{}',
    ip_address INET,
    user_agent TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ============================================================================
-- ROW LEVEL SECURITY POLICIES
-- ============================================================================

-- Enable RLS on all tables
ALTER TABLE public.users ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.user_preferences ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.prompt_runs ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.session_configs ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.exports ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.export_bundles ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.user_entitlements ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.feature_usage ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.user_activity ENABLE ROW LEVEL SECURITY;

-- Users can only see/edit their own data
CREATE POLICY "Users can view own profile" ON public.users
    FOR SELECT USING (auth.uid() = id);

CREATE POLICY "Users can update own profile" ON public.users
    FOR UPDATE USING (auth.uid() = id);

-- User preferences
CREATE POLICY "Users can manage own preferences" ON public.user_preferences
    FOR ALL USING (auth.uid() = user_id);

-- Prompt runs
CREATE POLICY "Users can view own runs" ON public.prompt_runs
    FOR SELECT USING (auth.uid() = user_id);

CREATE POLICY "Users can create runs" ON public.prompt_runs
    FOR INSERT WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can update own runs" ON public.prompt_runs
    FOR UPDATE USING (auth.uid() = user_id);

-- Session configs
CREATE POLICY "Users can manage own configs" ON public.session_configs
    FOR ALL USING (auth.uid() = user_id);

-- Exports
CREATE POLICY "Users can view own exports" ON public.exports
    FOR SELECT USING (auth.uid() = user_id);

CREATE POLICY "Users can create exports" ON public.exports
    FOR INSERT WITH CHECK (auth.uid() = user_id);

-- Export bundles
CREATE POLICY "Users can manage own bundles" ON public.export_bundles
    FOR ALL USING (auth.uid() = user_id);

-- User entitlements
CREATE POLICY "Users can view own entitlements" ON public.user_entitlements
    FOR SELECT USING (auth.uid() = user_id);

-- Feature usage
CREATE POLICY "Users can view own usage" ON public.feature_usage
    FOR SELECT USING (auth.uid() = user_id);

CREATE POLICY "Users can track own usage" ON public.feature_usage
    FOR INSERT WITH CHECK (auth.uid() = user_id);

-- User activity
CREATE POLICY "Users can view own activity" ON public.user_activity
    FOR SELECT USING (auth.uid() = user_id);

CREATE POLICY "System can log user activity" ON public.user_activity
    FOR INSERT WITH CHECK (auth.uid() = user_id);

-- Public read access for modules and entitlements
CREATE POLICY "Anyone can view modules" ON public.modules FOR SELECT USING (true);
CREATE POLICY "Anyone can view base entitlements" ON public.entitlements FOR SELECT USING (true);
CREATE POLICY "Anyone can view module usage stats" ON public.module_usage FOR SELECT USING (true);

-- ============================================================================
-- INDEXES FOR PERFORMANCE
-- ============================================================================

-- User lookup indexes
CREATE INDEX idx_users_email ON public.users(email);
CREATE INDEX idx_users_stripe_customer ON public.users(stripe_customer_id);
CREATE INDEX idx_users_plan ON public.users(plan);

-- Prompt runs indexes
CREATE INDEX idx_prompt_runs_user_id ON public.prompt_runs(user_id);
CREATE INDEX idx_prompt_runs_module_id ON public.prompt_runs(module_id);
CREATE INDEX idx_prompt_runs_created_at ON public.prompt_runs(created_at DESC);
CREATE INDEX idx_prompt_runs_status ON public.prompt_runs(status);

-- Export indexes
CREATE INDEX idx_exports_user_id ON public.exports(user_id);
CREATE INDEX idx_exports_created_at ON public.exports(created_at DESC);

-- Feature usage indexes
CREATE INDEX idx_feature_usage_user_date ON public.feature_usage(user_id, usage_date);
CREATE INDEX idx_feature_usage_feature_key ON public.feature_usage(feature_key);

-- Activity tracking
CREATE INDEX idx_user_activity_user_id ON public.user_activity(user_id);
CREATE INDEX idx_user_activity_created_at ON public.user_activity(created_at DESC);

-- ============================================================================
-- TRIGGERS & FUNCTIONS
-- ============================================================================

-- Update timestamp trigger function
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language plpgsql;

-- Apply update triggers
CREATE TRIGGER update_users_updated_at BEFORE UPDATE ON public.users
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_user_preferences_updated_at BEFORE UPDATE ON public.user_preferences
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_modules_updated_at BEFORE UPDATE ON public.modules
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_session_configs_updated_at BEFORE UPDATE ON public.session_configs
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- ============================================================================
-- INITIAL DATA SEEDING
-- ============================================================================

-- Default entitlements per plan
INSERT INTO public.entitlements (plan, feature_key, is_enabled, limit_value) VALUES
-- Pilot plan
('pilot', 'generator_access', true, NULL),
('pilot', 'basic_modules', true, 12),
('pilot', 'export_txt', true, NULL),
('pilot', 'export_md', true, NULL),
('pilot', 'monthly_runs', true, 50),

-- Pro plan  
('pro', 'generator_access', true, NULL),
('pro', 'all_modules', true, NULL),
('pro', 'gpt_testing', true, NULL),
('pro', 'export_txt', true, NULL),
('pro', 'export_md', true, NULL),
('pro', 'export_json', true, NULL),
('pro', 'export_pdf', true, NULL),
('pro', 'cloud_history', true, NULL),
('pro', 'monthly_runs', true, 500),

-- Enterprise plan
('enterprise', 'generator_access', true, NULL),
('enterprise', 'all_modules', true, NULL),
('enterprise', 'gpt_testing', true, NULL),
('enterprise', 'api_access', true, NULL),
('enterprise', 'export_txt', true, NULL),
('enterprise', 'export_md', true, NULL),
('enterprise', 'export_json', true, NULL),
('enterprise', 'export_pdf', true, NULL),
('enterprise', 'export_zip', true, NULL),
('enterprise', 'cloud_history', true, NULL),
('enterprise', 'custom_modules', true, NULL),
('enterprise', 'whitelabel', true, NULL),
('enterprise', 'monthly_runs', true, NULL); -- NULL = unlimited

-- ============================================================================
-- STORAGE BUCKETS SETUP
-- ============================================================================

-- Create storage buckets (run these in Supabase dashboard or via API)
/*
INSERT INTO storage.buckets (id, name, public) VALUES 
    ('exports', 'exports', false),
    ('templates', 'templates', true),
    ('assets', 'assets', true);

-- Storage policies
CREATE POLICY "Users can upload own exports" ON storage.objects
    FOR INSERT WITH CHECK (bucket_id = 'exports' AND auth.uid()::text = (storage.foldername(name))[1]);

CREATE POLICY "Users can view own exports" ON storage.objects
    FOR SELECT USING (bucket_id = 'exports' AND auth.uid()::text = (storage.foldername(name))[1]);

CREATE POLICY "Anyone can view public assets" ON storage.objects
    FOR SELECT USING (bucket_id IN ('templates', 'assets'));
*/





ARHITECTURA ONTOLOGICƒÇ ‚Äì 3 STRATURI
STRATUL 1: IDENTITATE & CONTROL

users + user_preferences ‚Üí nucleul identitƒÉ»õii
entitlements + user_entitlements ‚Üí gating-ul comercial
feature_usage ‚Üí telemetria monetizƒÉrii

STRATUL 2: PROCESARE SEMANTICƒÇ

modules (M01-M50) ‚Üí catalog static de unitƒÉ»õi industriale
prompt_runs ‚Üí execu»õii cu 7-D config + scoring
session_configs ‚Üí template-uri 7-D personalizate

STRATUL 3: ACTIVARE & EXPORT

exports + export_bundles ‚Üí materializarea valorii
Storage buckets ‚Üí persisten»õa artefactelor
Analytics ‚Üí feedback loop pentru optimizare

VECTORI CRITICI DE GATING:

Plan-based access ‚Üí entitlements determinƒÉ ce poate accesa userul
Usage tracking ‚Üí feature_usage numƒÉrƒÉ consumul per zi/lunƒÉ
Export control ‚Üí formatul .pdf/.zip locked pentru Pro+
API access ‚Üí doar Enterprise poate folosi /api/run/Mxx

STRATEGIA RLS (Row Level Security):
Fiecare user vede doar data sa. Zero cross-contamination. Politicile sunt granulare per ac»õiune (SELECT/INSERT/UPDATE).
PUNCTE DE PERFORMAN»öƒÇ:

Index compus pe (user_id, created_at DESC) pentru history queries
daily_usage pre-agregat pentru dashboards
module_usage cached pentru popularitate

INTEGRAREA STRIPE:
stripe_customer_id + subscription_status √Æn users table. Webhook-uri updateazƒÉ planul »ôi reseteazƒÉ limitele lunare.
URMƒÇTORUL VECTOR:
Define»ôti modulele M01-M10 √Æn lib/modules.ts. Fiecare modul are input schema + output template + KPI target.
Schema existƒÉ. Logica se construie»ôte deasupra ei.



// ============================================================================
// TELEMETRY & MONITORING SYSTEM - lib/telemetry.ts
// ============================================================================

import { createClient } from './supabase/client';
import { Module7DConfig } from './modules';

export interface RunMetrics {
  run_id: string;
  user_id: string;
  module_id: string;
  start_time: string;
  end_time?: string;
  status: 'pending' | 'running' | 'completed' | 'failed' | 'cancelled';
  execution_time_ms?: number;
  token_usage?: number;
  cost_usd?: number;
  kpi_scores?: Record<string, number>;
  error_message?: string;
  config_7d: Module7DConfig;
  cost_estimate: { tokens: number; usd: number };
}

export interface SystemMetrics {
  timestamp: string;
  metric_name: string;
  metric_value: number;
  tags: Record<string, string>;
  dimensions?: Record<string, any>;
}

export interface PerformanceMetrics {
  ttfb_ms: number; // Time to first byte
  tti_ms: number; // Time to interactive
  completion_rate: number;
  error_rate: number;
  avg_score: number;
  p95_response_time: number;
}

// ============================================================================
// RUN TRACKING
// ============================================================================

export async function logRunStart(
  runId: string,
  userId: string,
  moduleId: string,
  config7d: Module7DConfig,
  costEstimate: { tokens: number; usd: number }
): Promise<void> {
  const supabase = createClient();
  
  const runMetrics: Partial<RunMetrics> = {
    run_id: runId,
    user_id: userId,
    module_id: moduleId,
    start_time: new Date().toISOString(),
    status: 'running',
    config_7d: config7d,
    cost_estimate: costEstimate
  };

  // Insert into prompt_runs table
  const { error: dbError } = await supabase
    .from('prompt_runs')
    .insert({
      id: runId,
      user_id: userId,
      module_id: moduleId,
      session_config: config7d,
      status: 'running',
      created_at: runMetrics.start_time
    });

  if (dbError) {
    console.error('Failed to log run start:', dbError);
    // Continue execution - don't fail the run due to logging issues
  }

  // Send real-time metrics
  await sendMetric('run_started', 1, {
    module_id: moduleId,
    domain: config7d.domain,
    scale: config7d.scale,
    complexity: config7d.complexity
  });
}

export async function logRunComplete(
  runId: string,
  result: any,
  executionTimeMs: number
): Promise<void> {
  const supabase = createClient();
  const endTime = new Date().toISOString();
  
  // Update prompt_runs table
  const { error: dbError } = await supabase
    .from('prompt_runs')
    .update({
      status: 'completed',
      execution_time_ms: executionTimeMs,
      token_usage: result.token_usage,
      cost_usd: result.cost_usd,
      ai_score: result.kpi_scores?.overall || null,
      test_result: result.kpi_scores,
      completed_at: endTime
    })
    .eq('id', runId);

  if (dbError) {
    console.error('Failed to log run completion:', dbError);
  }

  // Send performance metrics
  await Promise.all([
    sendMetric('run_completed', 1, { run_id: runId }),
    sendMetric('execution_time_ms', executionTimeMs, { run_id: runId }),
    sendMetric('token_usage', result.token_usage, { run_id: runId }),
    sendMetric('cost_usd', result.cost_usd * 100, { run_id: runId }), // cents
    sendMetric('overall_score', result.kpi_scores?.overall || 0, { run_id: runId })
  ]);
}

export async function logRunError(
  runId: string,
  error: Error,
  executionTimeMs: number
): Promise<void> {
  const supabase = createClient();
  const endTime = new Date().toISOString();
  
  // Update prompt_runs table
  const { error: dbError } = await supabase
    .from('prompt_runs')
    .update({
      status: 'failed',
      execution_time_ms: executionTimeMs,
      error_message: error.message,
      completed_at: endTime
    })
    .eq('id', runId);

  if (dbError) {
    console.error('Failed to log run error:', dbError);
  }

  // Send error metrics
  await Promise.all([
    sendMetric('run_failed', 1, { run_id: runId, error_type: error.name }),
    sendMetric('execution_time_ms', executionTimeMs, { run_id: runId, status: 'failed' })
  ]);

  // Alert on critical errors
  if (isCritical(error)) {
    await sendAlert('critical_error', {
      run_id: runId,
      error_message: error.message,
      stack_trace: error.stack
    });
  }
}

// ============================================================================
// USER ACTIVITY TRACKING
// ============================================================================

export async function trackUserActivity(
  userId: string,
  action: string,
  resourceType?: string,
  resourceId?: string,
  metadata?: Record<string, any>,
  request?: Request
): Promise<void> {
  const supabase = createClient();
  
  // Extract IP and user agent safely
  let ipAddress: string | null = null;
  let userAgent: string | null = null;
  
  if (request) {
    // Get IP from various headers (handle proxies)
    ipAddress = request.headers.get('x-forwarded-for') ||
                request.headers.get('x-real-ip') ||
                request.headers.get('cf-connecting-ip') ||
                null;
    
    userAgent = request.headers.get('user-agent');
  }

  const { error } = await supabase
    .from('user_activity')
    .insert({
      user_id: userId,
      action,
      resource_type: resourceType,
      resource_id: resourceId,
      metadata: metadata || {},
      ip_address: ipAddress,
      user_agent: userAgent,
      created_at: new Date().toISOString()
    });

  if (error) {
    console.error('Failed to track user activity:', error);
  }

  // Send activity metrics
  await sendMetric('user_activity', 1, {
    action,
    resource_type: resourceType || 'unknown'
  });
}

// ============================================================================
// SYSTEM METRICS
// ============================================================================

export async function sendMetric(
  metricName: string,
  value: number,
  tags: Record<string, string> = {},
  dimensions?: Record<string, any>
): Promise<void> {
  const timestamp = new Date().toISOString();
  
  const metric: SystemMetrics = {
    timestamp,
    metric_name: metricName,
    metric_value: value,
    tags: {
      service: 'promptforge',
      environment: process.env.NODE_ENV || 'development',
      ...tags
    },
    dimensions
  };

  // Store in local metrics for aggregation
  await storeMetricLocal(metric);

  // Send to external monitoring if configured
  if (process.env.ENABLE_EXTERNAL_METRICS === 'true') {
    await sendMetricExternal(metric);
  }
}

async function storeMetricLocal(metric: SystemMetrics): Promise<void> {
  // This could write to a local time-series DB or queue for batch processing
  // For now, we'll use console logging in development
  if (process.env.NODE_ENV === 'development') {
    console.log(`[METRIC] ${metric.metric_name}: ${metric.metric_value}`, metric.tags);
  }
  
  // In production, you might want to:
  // - Write to InfluxDB, Prometheus, or CloudWatch
  // - Queue for batch processing to reduce overhead
  // - Use a metrics aggregation service
}

async function sendMetricExternal(metric: SystemMetrics): Promise<void> {
  // Example integrations:
  
  // DataDog
  if (process.env.DATADOG_API_KEY) {
    try {
      await fetch('https://api.datadoghq.com/api/v1/series', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'DD-API-KEY': process.env.DATADOG_API_KEY
        },
        body: JSON.stringify({
          series: [{
            metric: `promptforge.${metric.metric_name}`,
            points: [[Math.floor(Date.now() / 1000), metric.metric_value]],
            tags: Object.entries(metric.tags).map(([k, v]) => `${k}:${v}`)
          }]
        })
      });
    } catch (error) {
      console.error('Failed to send DataDog metric:', error);
    }
  }

  // PostHog
  if (process.env.POSTHOG_API_KEY) {
    try {
      await fetch('https://app.posthog.com/capture/', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          api_key: process.env.POSTHOG_API_KEY,
          event: metric.metric_name,
          properties: {
            ...metric.tags,
            value: metric.metric_value,
            timestamp: metric.timestamp
          }
        })
      });
    } catch (error) {
      console.error('Failed to send PostHog metric:', error);
    }
  }
}

// ============================================================================
// PERFORMANCE MONITORING
// ============================================================================

export async function calculateDailyMetrics(date: string = new Date().toISOString().split('T')[0]): Promise<PerformanceMetrics> {
  const supabase = createClient();
  
  const { data: dailyRuns } = await supabase
    .from('prompt_runs')
    .select('execution_time_ms, ai_score, status, created_at')
    .gte('created_at', `${date}T00:00:00Z`)
    .lt('created_at', `${date}T23:59:59Z`);

  if (!dailyRuns || dailyRuns.length === 0) {
    return {
      ttfb_ms: 0,
      tti_ms: 0,
      completion_rate: 0,
      error_rate: 0,
      avg_score: 0,
      p95_response_time: 0
    };
  }

  const completed = dailyRuns.filter(run => run.status === 'completed');
  const failed = dailyRuns.filter(run => run.status === 'failed');
  const executionTimes = completed
    .map(run => run.execution_time_ms)
    .filter(time => time != null)
    .sort((a, b) => a - b);
  
  const avgScore = completed.length > 0 
    ? completed.reduce((sum, run) => sum + (run.ai_score || 0), 0) / completed.length
    : 0;

  const p95Index = Math.floor(executionTimes.length * 0.95);
  const p95ResponseTime = executionTimes.length > 0 ? executionTimes[p95Index] || 0 : 0;

  return {
    ttfb_ms: executionTimes.length > 0 ? Math.min(...executionTimes) : 0,
    tti_ms: executionTimes.length > 0 ? executionTimes.reduce((sum, time) => sum + time, 0) / executionTimes.length : 0,
    completion_rate: dailyRuns.length > 0 ? completed.length / dailyRuns.length : 0,
    error_rate: dailyRuns.length > 0 ? failed.length / dailyRuns.length : 0,
    avg_score: avgScore,
    p95_response_time: p95ResponseTime
  };
}

export async function updateDailyAggregates(): Promise<void> {
  const supabase = createClient();
  const yesterday = new Date();
  yesterday.setDate(yesterday.getDate() - 1);
  const dateStr = yesterday.toISOString().split('T')[0];
  
  const metrics = await calculateDailyMetrics(dateStr);
  
  // Get additional stats
  const { data: userCount } = await supabase
    .from('prompt_runs')
    .select('user_id', { count: 'exact' })
    .gte('created_at', `${dateStr}T00:00:00Z`)
    .lt('created_at', `${dateStr}T23:59:59Z`);

  const { data: exportCount } = await supabase
    .from('exports')
    .select('id', { count: 'exact' })
    .gte('created_at', `${dateStr}T00:00:00Z`)
    .lt('created_at', `${dateStr}T23:59:59Z`);

  // Calculate revenue (rough estimate)
  const { data: subscriptions } = await supabase
    .from('users')
    .select('plan')
    .neq('plan', 'pilot');

  const planRevenue = { pro: 29, enterprise: 99 }; // monthly values
  const dailyRevenue = (subscriptions || []).reduce((sum, sub) => {
    return sum + (planRevenue[sub.plan as keyof typeof planRevenue] || 0) / 30;
  }, 0);

  // Upsert daily aggregates
  await supabase
    .from('daily_usage')
    .upsert({
      date: dateStr,
      total_runs: userCount?.length || 0,
      total_users: new Set((userCount || []).map(r => r.user_id)).size,
      total_exports: exportCount?.length || 0,
      avg_score: Math.round(metrics.avg_score * 10) / 10,
      revenue_usd: Math.round(dailyRevenue * 100) / 100
    }, { onConflict: 'date' });
}

// ============================================================================
// ALERTING
// ============================================================================

interface Alert {
  type: 'error' | 'warning' | 'info' | 'critical';
  title: string;
  message: string;
  metadata?: Record<string, any>;
  timestamp: string;
}

export async function sendAlert(
  type: Alert['type'],
  data: Omit<Alert, 'type' | 'timestamp'>
): Promise<void> {
  const alert: Alert = {
    type,
    timestamp: new Date().toISOString(),
    ...data
  };

  console.error(`[ALERT:${type.toUpperCase()}]`, alert);

  // Send to configured alert channels
  await Promise.all([
    sendSlackAlert(alert),
    sendEmailAlert(alert),
    storeAlertInDB(alert)
  ]);
}

async function sendSlackAlert(alert: Alert): Promise<void> {
  if (!process.env.SLACK_WEBHOOK_URL) return;

  const color = {
    error: 'danger',
    critical: 'danger', 
    warning: 'warning',
    info: 'good'
  };

  try {
    await fetch(process.env.SLACK_WEBHOOK_URL, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        attachments: [{
          color: color[alert.type],
          title: `üö® ${alert.title}`,
          text: alert.message,
          fields: alert.metadata ? Object.entries(alert.metadata).map(([key, value]) => ({
            title: key,
            value: typeof value === 'object' ? JSON.stringify(value) : String(value),
            short: true
          })) : [],
          ts: Math.floor(Date.now() / 1000)
        }]
      })
    });
  } catch (error) {
    console.error('Failed to send Slack alert:', error);
  }
}

async function sendEmailAlert(alert: Alert): Promise<void> {
  if (alert.type === 'critical' && process.env.ALERT_EMAIL) {
    // In production, integrate with email service (SendGrid, SES, etc.)
    console.log(`Would send critical alert email to ${process.env.ALERT_EMAIL}`);
  }
}

async function storeAlertInDB(alert: Alert): Promise<void> {
  // Store alerts in database for historical tracking
  // This would be useful for alert fatigue analysis and pattern recognition
}

// ============================================================================
// HEALTH CHECKS
// ============================================================================

export async function checkSystemHealth(): Promise<{
  status: 'healthy' | 'degraded' | 'unhealthy';
  checks: Record<string, { status: 'pass' | 'fail'; message?: string; duration_ms: number }>;
}> {
  const checks: Record<string, { status: 'pass' | 'fail'; message?: string; duration_ms: number }> = {};
  
  // Database connectivity
  const dbStart = Date.now();
  try {
    const supabase = createClient();
    await supabase.from('users').select('id').limit(1);
    checks.database = { status: 'pass', duration_ms: Date.now() - dbStart };
  } catch (error) {
    checks.database = { 
      status: 'fail', 
      message: error instanceof Error ? error.message : 'Unknown error',
      duration_ms: Date.now() - dbStart 
    };
  }

  // OpenAI API connectivity  
  const openaiStart = Date.now();
  try {
    // Simple API test
    await fetch('https://api.openai.com/v1/models', {
      headers: { 'Authorization': `Bearer ${process.env.OPENAI_API_KEY}` }
    });
    checks.openai = { status: 'pass', duration_ms: Date.now() - openaiStart };
  } catch (error) {
    checks.openai = { 
      status: 'fail', 
      message: 'OpenAI API unreachable',
      duration_ms: Date.now() - openaiStart 
    };
  }

  // Recent error rate check
  const errorStart = Date.now();
  try {
    const recentMetrics = await calculateDailyMetrics();
    checks.error_rate = { 
      status: recentMetrics.error_rate < 0.05 ? 'pass' : 'fail',
      message: `Error rate: ${(recentMetrics.error_rate * 100).toFixed(2)}%`,
      duration_ms: Date.now() - errorStart
    };
  } catch (error) {
    checks.error_rate = { status: 'fail', message: 'Cannot calculate metrics', duration_ms: Date.now() - errorStart };
  }

  const failedChecks = Object.values(checks).filter(check => check.status === 'fail').length;
  const status = failedChecks === 0 ? 'healthy' : failedChecks === 1 ? 'degraded' : 'unhealthy';

  return { status, checks };
}

// ============================================================================
// UTILITY FUNCTIONS
// ============================================================================

function isCritical(error: Error): boolean {
  const criticalPatterns = [
    'ENOTFOUND',
    'ECONNREFUSED', 
    'timeout',
    'out of memory',
    'permission denied',
    'authentication failed'
  ];
  
  return criticalPatterns.some(pattern => 
    error.message.toLowerCase().includes(pattern.toLowerCase())
  );
}

export function startMetricsAggregation(): void {
  // Run daily aggregation at 1 AM UTC
  const schedule = '0 1 * * *'; // cron format
  
  if (typeof window === 'undefined') { // server-side only
    // In production, use a proper cron job or scheduled task
    console.log('Metrics aggregation would be scheduled:', schedule);
  }
}

// Export configuration for monitoring dashboard
export const MONITORING_CONFIG = {
  metrics: [
    { name: 'run_started', type: 'counter', description: 'Total runs started' },
    { name: 'run_completed', type: 'counter', description: 'Total runs completed' },
    { name: 'run_failed', type: 'counter', description: 'Total runs failed' },
    { name: 'execution_time_ms', type: 'histogram', description: 'Execution time distribution' },
    { name: 'token_usage', type: 'histogram', description: 'Token usage distribution' },
    { name: 'cost_usd', type: 'histogram', description: 'Cost distribution in cents' },
    { name: 'overall_score', type: 'histogram', description: 'Quality score distribution' },
    { name: 'user_activity', type: 'counter', description: 'User activity events' }
  ],
  alerts: [
    { name: 'high_error_rate', threshold: '5%', window: '5m' },
    { name: 'slow_response_time', threshold: '10s', window: '1m' },
    { name: 'api_quota_exceeded', threshold: '90%', window: '1h' },
    { name: 'database_connection_failed', threshold: '1 occurrence', window: '1m' }
  ]
};







{
  "package.json": {
    "name": "promptforge",
    "version": "1.0.0",
    "description": "Industrial prompt generation platform with 50 modules and 7D configuration",
    "main": "index.js",
    "scripts": {
      "dev": "next dev",
      "build": "next build",
      "start": "next start",
      "lint": "next lint",
      "type-check": "tsc --noEmit",
      "test": "jest",
      "test:watch": "jest --watch",
      "test:e2e": "playwright test",
      "seed": "tsx scripts/seed-modules.ts",
      "migrate": "tsx scripts/migrate.ts",
      "build-sitemap": "tsx scripts/build-sitemap.ts",
      "format": "prettier --write .",
      "format:check": "prettier --check ."
    },
    "dependencies": {
      "next": "^14.0.0",
      "react": "^18.2.0",
      "react-dom": "^18.2.0",
      "@supabase/supabase-js": "^2.38.0",
      "@supabase/auth-helpers-nextjs": "^0.8.7",
      "stripe": "^14.0.0",
      "@stripe/stripe-js": "^2.1.0",
      "openai": "^4.20.0",
      "zod": "^3.22.4",
      "@radix-ui/react-dialog": "^1.0.5",
      "@radix-ui/react-select": "^2.0.0",
      "@radix-ui/react-checkbox": "^1.0.4",
      "@radix-ui/react-tabs": "^1.0.4",
      "@radix-ui/react-slider": "^1.1.2",
      "@radix-ui/react-label": "^2.0.2",
      "@radix-ui/react-toast": "^1.1.5",
      "lucide-react": "^0.292.0",
      "class-variance-authority": "^0.7.0",
      "clsx": "^2.0.0",
      "tailwind-merge": "^2.0.0",
      "@hookform/resolvers": "^3.3.2",
      "react-hook-form": "^7.47.0",
      "date-fns": "^2.30.0",
      "lodash": "^4.17.21",
      "uuid": "^9.0.1",
      "crypto-js": "^4.2.0",
      "@sentry/nextjs": "^7.81.1",
      "posthog-js": "^1.90.0"
    },
    "devDependencies": {
      "typescript": "^5.2.2",
      "@types/node": "^20.8.7",
      "@types/react": "^18.2.31",
      "@types/react-dom": "^18.2.14",
      "@types/lodash": "^4.14.201",
      "@types/uuid": "^9.0.6",
      "@types/crypto-js": "^4.2.1",
      "eslint": "^8.52.0",
      "eslint-config-next": "^14.0.0",
      "prettier": "^3.0.3",
      "tailwindcss": "^3.3.5",
      "@tailwindcss/typography": "^0.5.10",
      "autoprefixer": "^10.4.16",
      "postcss": "^8.4.31",
      "jest": "^29.7.0",
      "@testing-library/react": "^13.4.0",
      "@testing-library/jest-dom": "^6.1.4",
      "@playwright/test": "^1.40.0",
      "tsx": "^4.1.2"
    },
    "keywords": ["ai", "prompts", "nextjs", "saas", "prompt-engineering"],
    "author": "Vadim Cu»ônir",
    "license": "MIT"
  },
  
  ".env.example": {
    "description": "Environment variables template",
    "content": "# Supabase Configuration\nNEXT_PUBLIC_SUPABASE_URL=your_supabase_project_url\nNEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key\nSUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key\n\n# OpenAI Configuration\nOPENAI_API_KEY=your_openai_api_key\n\n# Stripe Configuration\nNEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY=your_stripe_publishable_key\nSTRIPE_SECRET_KEY=your_stripe_secret_key\nSTRIPE_WEBHOOK_SECRET=your_stripe_webhook_secret\n\n# App Configuration\nNEXT_PUBLIC_APP_URL=http://localhost:3000\nNODE_ENV=development\n\n# Optional: Analytics & Monitoring\nNEXT_PUBLIC_POSTHOG_KEY=your_posthog_key\nNEXT_PUBLIC_POSTHOG_HOST=https://app.posthog.com\nSENTRY_DSN=your_sentry_dsn\nDATADOG_API_KEY=your_datadog_key\n\n# Optional: Alerts\nSLACK_WEBHOOK_URL=your_slack_webhook\nALERT_EMAIL=alerts@yourcompany.com\n\n# Feature Flags\nENABLE_EXTERNAL_METRICS=false\nENABLE_DEBUG_LOGGING=true"
  },
  
  "next.config.js": {
    "/** @type {import('next').NextConfig} */": null,
    "const nextConfig": {
      "experimental": {
        "appDir": true
      },
      "images": {
        "domains": ["supabase.storage.com"]
      },
      "env": {
        "CUSTOM_KEY": "value"
      },
      "async rewrites()": [
        {
          "source": "/api/webhooks/stripe",
          "destination": "/api/webhooks/stripe"
        }
      ],
      "async headers()": [
        {
          "source": "/api/:path*",
          "headers": [
            {
              "key": "Access-Control-Allow-Origin",
              "value": "*"
            },
            {
              "key": "Access-Control-Allow-Methods", 
              "value": "GET, POST, PUT, DELETE, OPTIONS"
            },
            {
              "key": "Access-Control-Allow-Headers",
              "value": "Content-Type, Authorization"
            }
          ]
        }
      ]
    },
    "module.exports": "nextConfig"
  },
  
  "tailwind.config.js": {
    "/** @type {import('tailwindcss').Config} */": null,
    "module.exports": {
      "darkMode": ["class"],
      "content": [
        "./pages/**/*.{ts,tsx}",
        "./components/**/*.{ts,tsx}",
        "./app/**/*.{ts,tsx}",
        "./src/**/*.{ts,tsx}"
      ],
      "theme": {
        "container": {
          "center": true,
          "padding": "2rem",
          "screens": {
            "2xl": "1400px"
          }
        },
        "extend": {
          "colors": {
            "border": "hsl(var(--border))",
            "input": "hsl(var(--input))",
            "ring": "hsl(var(--ring))",
            "background": "hsl(var(--background))",
            "foreground": "hsl(var(--foreground))",
            "primary": {
              "DEFAULT": "hsl(var(--primary))",
              "foreground": "hsl(var(--primary-foreground))"
            },
            "secondary": {
              "DEFAULT": "hsl(var(--secondary))",
              "foreground": "hsl(var(--secondary-foreground))"
            },
            "destructive": {
              "DEFAULT": "hsl(var(--destructive))",
              "foreground": "hsl(var(--destructive-foreground))"
            },
            "muted": {
              "DEFAULT": "hsl(var(--muted))",
              "foreground": "hsl(var(--muted-foreground))"
            },
            "accent": {
              "DEFAULT": "hsl(var(--accent))",
              "foreground": "hsl(var(--accent-foreground))"
            },
            "popover": {
              "DEFAULT": "hsl(var(--popover))",
              "foreground": "hsl(var(--popover-foreground))"
            },
            "card": {
              "DEFAULT": "hsl(var(--card))",
              "foreground": "hsl(var(--card-foreground))"
            }
          },
          "borderRadius": {
            "lg": "var(--radius)",
            "md": "calc(var(--radius) - 2px)",
            "sm": "calc(var(--radius) - 4px)"
          },
          "keyframes": {
            "accordion-down": {
              "from": { "height": "0" },
              "to": { "height": "var(--radix-accordion-content-height)" }
            },
            "accordion-up": {
              "from": { "height": "var(--radix-accordion-content-height)" },
              "to": { "height": "0" }
            }
          },
          "animation": {
            "accordion-down": "accordion-down 0.2s ease-out",
            "accordion-up": "accordion-up 0.2s ease-out"
          }
        }
      },
      "plugins": ["require('tailwindcss-animate')", "require('@tailwindcss/typography')"]
    }
  },
  
  "tsconfig.json": {
    "compilerOptions": {
      "target": "es5",
      "lib": ["dom", "dom.iterable", "esnext"],
      "allowJs": true,
      "skipLibCheck": true,
      "strict": true,
      "noEmit": true,
      "esModuleInterop": true,
      "module": "esnext",
      "moduleResolution": "bundler",
      "resolveJsonModule": true,
      "isolatedModules": true,
      "jsx": "preserve",
      "incremental": true,
      "plugins": [
        {
          "name": "next"
        }
      ],
      "baseUrl": ".",
      "paths": {
        "@/*": ["./"],
        "@/components/*": ["./components/*"],
        "@/lib/*": ["./lib/*"],
        "@/hooks/*": ["./hooks/*"],
        "@/types/*": ["./types/*"]
      }
    },
    "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
    "exclude": ["node_modules"]
  },
  
  "scripts/seed-modules.ts": "import { createClient } from '@supabase/supabase-js';\nimport { MODULES_CATALOG } from '../lib/modules';\n\nconst supabase = createClient(\n  process.env.NEXT_PUBLIC_SUPABASE_URL!,\n  process.env.SUPABASE_SERVICE_ROLE_KEY!\n);\n\nasync function seedModules() {\n  console.log('Seeding modules...');\n  \n  for (const module of Object.values(MODULES_CATALOG)) {\n    const { error } = await supabase\n      .from('modules')\n      .upsert({\n        id: module.id,\n        title: module.name,\n        description: module.description,\n        vector: module.vectors[0], // Primary vector\n        difficulty: module.difficulty,\n        estimated_tokens: module.estimated_tokens,\n        input_schema: module.input_schema,\n        output_template: module.output_template,\n        guardrails: module.guardrails.policy,\n        kpi_target: JSON.stringify(module.kpi),\n        sample_output: module.sample_output,\n        is_active: module.is_active,\n        requires_plan: module.requires_plan\n      }, { onConflict: 'id' });\n      \n    if (error) {\n      console.error(`Error seeding ${module.id}:`, error);\n    } else {\n      console.log(`‚úì Seeded ${module.id}`);\n    }\n  }\n  \n  console.log('Module seeding complete!');\n}\n\nseedModules().catch(console.error);",
  
  "scripts/migrate.ts": "import { createClient } from '@supabase/supabase-js';\nimport fs from 'fs';\nimport path from 'path';\n\nconst supabase = createClient(\n  process.env.NEXT_PUBLIC_SUPABASE_URL!,\n  process.env.SUPABASE_SERVICE_ROLE_KEY!\n);\n\nasync function runMigrations() {\n  console.log('Running database migrations...');\n  \n  // Read SQL schema file\n  const schemaPath = path.join(process.cwd(), 'db', 'schema.sql');\n  \n  if (!fs.existsSync(schemaPath)) {\n    console.error('Schema file not found at:', schemaPath);\n    process.exit(1);\n  }\n  \n  const schema = fs.readFileSync(schemaPath, 'utf8');\n  \n  // Split by statements and execute\n  const statements = schema\n    .split(';')\n    .map(s => s.trim())\n    .filter(s => s.length > 0 && !s.startsWith('--'));\n  \n  for (const statement of statements) {\n    try {\n      const { error } = await supabase.rpc('exec_sql', { sql: statement });\n      if (error) {\n        console.error('Migration error:', error);\n      }\n    } catch (err) {\n      console.warn('Statement skipped (likely already exists):', statement.substring(0, 50) + '...');\n    }\n  }\n  \n  console.log('Migrations complete!');\n}\n\nrunMigrations().catch(console.error);",
  
  "README.md": "# PromptForge - Industrial Prompt Generation Platform\n\nüè≠ **chatgpt-prompting.com** - The operational prompt generator with 50 modules, 7-dimensional configuration, and enterprise features.\n\n## Features\n\n- **50 Industrial Modules** (M01-M50) covering strategic, content, technical domains\n- **7-Dimensional Configuration** - Domain, Scale, Urgency, Complexity, Resources, Application, Output Format\n- **Real-time GPT Testing** - Validate prompts with live AI scoring\n- **Multi-format Export** - TXT, MD, JSON, PDF, ZIP bundles\n- **Enterprise Features** - API access, custom modules, whitelabel options\n- **Usage Analytics** - Track performance, costs, and optimization opportunities\n\n## Quick Start\n\n1. **Clone & Install**\n   ```bash\n   git clone https://github.com/your-org/promptforge\n   cd promptforge\n   npm install\n   ```\n\n2. **Environment Setup**\n   ```bash\n   cp .env.example .env.local\n   # Edit .env.local with your API keys\n   ```\n\n3. **Database Setup**\n   ```bash\n   npm run migrate\n   npm run seed\n   ```\n\n4. **Development Server**\n   ```bash\n   npm run dev\n   ```\n\n## Architecture\n\n- **Frontend**: Next.js 14 + React 18 + Tailwind CSS\n- **Backend**: Next.js API Routes + Supabase\n- **Database**: PostgreSQL with Row Level Security\n- **Payments**: Stripe subscriptions\n- **AI**: OpenAI GPT-4 for optimization and testing\n- **Monitoring**: Sentry + PostHog + DataDog\n\n## Module System\n\nEach module follows a strict contract:\n- **Input Schema** - Validated parameters\n- **7D Configuration** - Contextual adaptation\n- **Output Template** - Consistent structure\n- **KPI Scoring** - Quality metrics\n- **Guardrails** - Safety and compliance\n\n## Deployment\n\n1. **Vercel** (Recommended)\n   - Connect GitHub repository\n   - Set environment variables\n   - Deploy automatically on push\n\n2. **Custom Server**\n   ```bash\n   npm run build\n   npm run start\n   ```\n\n## API Documentation\n\n### Execute Module\n```bash\nPOST /api/run/M01\n{\n  \"config_7d\": {\n    \"domain\": \"b2b_saas\",\n    \"scale\": \"company\",\n    \"urgency\": \"medium\",\n    \"complexity\": \"complex\",\n    \"resources\": \"standard\",\n    \"application\": \"client\",\n    \"output_format\": \"markdown\"\n  },\n  \"module_params\": {\n    \"subject\": \"Customer onboarding process\",\n    \"level\": \"intermediate\",\n    \"context\": \"B2B SaaS platform with 100+ users\"\n  }\n}\n```\n\n### Export Results\n```bash\nPOST /api/export\n{\n  \"run_id\": \"uuid\",\n  \"formats\": [\"md\", \"pdf\", \"json\"],\n  \"include_telemetry\": true\n}\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create feature branch: `git checkout -b feature/new-module`\n3. Follow module contract in `/lib/modules.ts`\n4. Add tests for new functionality\n5. Submit pull request\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n## Support\n\n- üìß **Email**: support@chatgpt-prompting.com\n- üìñ **Docs**: /docs/api\n- üêõ **Issues**: GitHub Issues\n- üí¨ **Discord**: [PromptForge Community](https://discord.gg/promptforge)"
}